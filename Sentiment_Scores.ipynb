{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import length, col\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = spark.read.csv(\"file:///C:\\spark/spark/bin/all-the-news-2-1.csv\", inferSchema = True,header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news_data.select(\"Date\",\"Title\",\"Article\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex & Lowering Data before selecting stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_removed_punctuations = news.select('*', (lower(regexp_replace('Article', \"[^a-zA-Z\\\\s]\", \"\")).alias('Regex Removed & Lower Cased Articles')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting stock news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterStock(stockName, df):\n",
    "    if stockName == \"apple\":\n",
    "        return df.filter(df['Regex Removed & Lower Cased Articles'].contains(stockName) & ~df['Regex Removed & Lower Cased Articles'].contains(\"fruit\"))\n",
    "    elif stockName == \"amazon\":\n",
    "        return df.filter(df['Regex Removed & Lower Cased Articles'].contains(stockName) & ~df['Regex Removed & Lower Cased Articles'].contains(\"forest\"))\n",
    "    else:\n",
    "        return df.filter(df['Regex Removed & Lower Cased Articles'].contains(stockName))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_news = filterStock(\"apple\",news_removed_punctuations)\n",
    "AMZN_news = filterStock('amazon',news_removed_punctuations)\n",
    "NFLX_news = filterStock('netflix',news_removed_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NFLX news: 35213\n",
      "Number of AMZN news: 66259\n",
      "Number of AAPL news: 86935\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of NFLX news: {}\\nNumber of AMZN news: {}\\nNumber of AAPL news: {}\" \\\n",
    "     .format(NFLX_news.count(),AMZN_news.count(),AAPL_news.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|2017-09-26 09:00:02|Where the softwar...|Software companie...|                software companie...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAPL_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|2018-11-24 00:00:00|On Black Friday, ...|NEW YORK (Reuters...|                new york reuters ...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "|2017-11-30 20:12:02|Forget Facebook, ...|\"Facebook, Amazon...|                facebook amazon n...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_news.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizating Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    tokenizer = Tokenizer(inputCol=\"Regex Removed & Lower Cased Articles\", outputCol=\"Tokenized Articles\")\n",
    "    return tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_news = tokenize(AAPL_news)\n",
    "AMZN_news = tokenize(AMZN_news)\n",
    "NFLX_news = tokenize(NFLX_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|2017-09-26 09:00:02|Where the softwar...|Software companie...|                software companie...|[software, compan...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAPL_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|2018-11-24 00:00:00|On Black Friday, ...|NEW YORK (Reuters...|                new york reuters ...|[new, york, reute...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "|2017-11-30 20:12:02|Forget Facebook, ...|\"Facebook, Amazon...|                facebook amazon n...|[facebook, amazon...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_news.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_remove(df):\n",
    "    remover = StopWordsRemover(inputCol=\"Tokenized Articles\", outputCol=\"Articles without stop words\")\n",
    "    return remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_news = stop_word_remove(AAPL_news)\n",
    "AMZN_news = stop_word_remove(AMZN_news)\n",
    "NFLX_news = stop_word_remove(NFLX_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|Articles without stop words|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|2017-09-26 09:00:02|Where the softwar...|Software companie...|                software companie...|[software, compan...|       [software, compan...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAPL_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|Articles without stop words|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|2018-11-24 00:00:00|On Black Friday, ...|NEW YORK (Reuters...|                new york reuters ...|[new, york, reute...|       [new, york, reute...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_news.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|               Date|               Title|             Article|Regex Removed & Lower Cased Articles|  Tokenized Articles|Articles without stop words|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "|2017-11-30 20:12:02|Forget Facebook, ...|\"Facebook, Amazon...|                facebook amazon n...|[facebook, amazon...|       [facebook, amazon...|\n",
      "+-------------------+--------------------+--------------------+------------------------------------+--------------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_news.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def countVector(dataf):\n",
    "    cv = CountVectorizer(inputCol=\"Articles without stop words\", outputCol=\"Count Vector\", vocabSize=200000, minDF=2.0)\n",
    "    model = cv.fit(dataf)\n",
    "    return model.transform(dataf)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "countvector_df = countVector(stop_words_removed_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count_vector = countvector_df.select(\"Count Vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join tokenized articles back to feed it to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_news = AAPL_news.select(\"Date\",\"Articles without stop words\",).rdd.map(tuple).map(lambda x: (x[0],x[1]))\n",
    "AMZN_news = AMZN_news.select(\"Date\",\"Articles without stop words\",).rdd.map(tuple).map(lambda x: (x[0],x[1]))\n",
    "NFLX_news = NFLX_news.select(\"Date\",\"Articles without stop words\",).rdd.map(tuple).map(lambda x: (x[0],x[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function returns a sentiment score for each article\n",
    "def sentScores(x):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    empty_list = []\n",
    "    for element in x:\n",
    "        empty_list.append(element)\n",
    "    sentences = \" \".join(empty_list)\n",
    "    score = analyzer.polarity_scores(sentences)\n",
    "    dict = score\n",
    "    return dict['compound']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding sentiment scores for each article and putting to rdd\n",
    "AAPL_sentimentRDD = AAPL_news.map(lambda x: (x[0],sentScores(x[1])))\n",
    "AMZN_sentimentRDD = AMZN_news.map(lambda x: (x[0],sentScores(x[1])))\n",
    "NFLX_sentimentRDD = NFLX_news.map(lambda x: (x[0],sentScores(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to convert back to dataframe\n",
    "def createDf(x):\n",
    "    d = {}\n",
    "    for i in range(len(x)):\n",
    "        d[str(i)] = x[i]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL_df = AAPL_sentimentRDD.map(lambda x: Row(**createDf(x))).toDF()\n",
    "AMZN_df = AMZN_sentimentRDD.map(lambda x: Row(**createDf(x))).toDF()\n",
    "NFLX_df = NFLX_sentimentRDD.map(lambda x: Row(**createDf(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|                  0|      1|\n",
      "+-------------------+-------+\n",
      "|2017-09-26 09:00:02| 0.9923|\n",
      "|2018-12-27 00:00:00|-0.8905|\n",
      "|2019-03-24 00:00:00|-0.9679|\n",
      "|2016-03-09 00:00:00| -0.121|\n",
      "|2018-08-31 00:00:00| 0.9914|\n",
      "+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAPL_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|                  0|     1|\n",
      "+-------------------+------+\n",
      "|2018-11-24 00:00:00|0.9859|\n",
      "|2017-11-30 20:12:02|0.9973|\n",
      "|2018-08-31 00:00:00|0.9914|\n",
      "|2016-08-23 00:00:00|0.9911|\n",
      "|2019-01-09 19:31:30|   1.0|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|                  0|     1|\n",
      "+-------------------+------+\n",
      "|2017-11-30 20:12:02|0.9973|\n",
      "|2019-05-22 17:20:00|0.9979|\n",
      "|2019-01-09 19:31:30|   1.0|\n",
      "|2019-01-16 12:48:00|   0.0|\n",
      "|2018-11-02 22:50:07|0.9897|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting back to dataframe and renamed the columns as date and sentiment score\n",
    "NFLX_sentiment_df = sql.createDataFrame(NFLX_sentimentRDD).withColumnRenamed(\"_1\",\"Date\").withColumnRenamed(\"_2\",\"Sentiment_Score\")\n",
    "AMZN_sentiment_df = sql.createDataFrame(AMZN_sentimentRDD).withColumnRenamed(\"_1\",\"Date\").withColumnRenamed(\"_2\",\"Sentiment_Score\")\n",
    "AAPL_sentiment_df = sql.createDataFrame(AAPL_sentimentRDD).withColumnRenamed(\"_1\",\"Date\").withColumnRenamed(\"_2\",\"Sentiment_Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|               Date|Sentiment_Score|\n",
      "+-------------------+---------------+\n",
      "|2017-11-30 20:12:02|         0.9973|\n",
      "|2019-05-22 17:20:00|         0.9979|\n",
      "|2019-01-09 19:31:30|            1.0|\n",
      "|2019-01-16 12:48:00|            0.0|\n",
      "|2018-11-02 22:50:07|         0.9897|\n",
      "+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_sentiment_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Removing Time from Date Column\n",
    "def fixdate(df):\n",
    "    split_col = pyspark.sql.functions.split(df['Date'], ' ')\n",
    "    date_extracted = df.withColumn('Date_1', split_col.getItem(0))\n",
    "    date_extracted = date_extracted.drop(\"Date\")\n",
    "    return date_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFLX_fixdate = fixdate(NFLX_sentiment_df).withColumnRenamed(\"Date_1\",\"Date\")\n",
    "AMZN_fixdate = fixdate(AMZN_sentiment_df).withColumnRenamed(\"Date_1\",\"Date\")\n",
    "AAPL_fixdate = fixdate(AAPL_sentiment_df).withColumnRenamed(\"Date_1\",\"Date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|Sentiment_Score|      Date|\n",
      "+---------------+----------+\n",
      "|         0.9859|2018-11-24|\n",
      "|         0.9973|2017-11-30|\n",
      "|         0.9914|2018-08-31|\n",
      "|         0.9911|2016-08-23|\n",
      "|            1.0|2019-01-09|\n",
      "+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_fixdate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping news that has been published on the same day. Summing the sentiment values for day.\n",
    "NFLX_groupby = NFLX_fixdate.groupBy('Date').agg(_sum('Sentiment_Score').alias('SentimentScores_Summed'))\n",
    "AMZN_groupby = AMZN_fixdate.groupBy('Date').agg(_sum('Sentiment_Score').alias('SentimentScores_Summed'))\n",
    "AAPL_groupby = AAPL_fixdate.groupBy('Date').agg(_sum('Sentiment_Score').alias('SentimentScores_Summed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------+\n",
      "|      Date|SentimentScores_Summed|\n",
      "+----------+----------------------+\n",
      "|2016-08-17|    13.309399999999998|\n",
      "|2017-12-05|     4.066799999999999|\n",
      "|2017-05-14|                3.9365|\n",
      "|2019-08-08|    19.849600000000002|\n",
      "|2019-08-22|    24.607599999999998|\n",
      "+----------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_groupby.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting dfs to rdd to make operation\n",
    "NFLX_scores_rdd  = NFLX_groupby.rdd.map(tuple)\n",
    "AMZN_scores_rdd  = AMZN_groupby.rdd.map(tuple)\n",
    "AAPL_scores_rdd  = AAPL_groupby.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting the \"00:00:00\" to convert it to a timestamp\n",
    "NFLX_scores_time = NFLX_scores_rdd.map(lambda x: (x[0] + \" 00:00:00\", x[1]))\n",
    "AMZN_scores_time = AMZN_scores_rdd.map(lambda x: (x[0] + \" 00:00:00\", x[1]))\n",
    "AAPL_scores_time = AAPL_scores_rdd.map(lambda x: (x[0] + \" 00:00:00\", x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016-08-17 00:00:00', 13.309399999999998),\n",
       " ('2017-12-05 00:00:00', 4.066799999999999),\n",
       " ('2017-05-14 00:00:00', 3.9365),\n",
       " ('2019-08-08 00:00:00', 19.849600000000002),\n",
       " ('2019-08-22 00:00:00', 24.607599999999998)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NFLX_scores_time.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting every stock back to a dataframe\n",
    "NFLX_scores_time_df = NFLX_scores_time.map(lambda x: Row(**createDf(x))).toDF()\n",
    "AMZN_scores_time_df = AMZN_scores_time.map(lambda x: Row(**createDf(x))).toDF()\n",
    "AAPL_scores_time_df = AAPL_scores_time.map(lambda x: Row(**createDf(x))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|                  0|                 1|\n",
      "+-------------------+------------------+\n",
      "|2016-08-17 00:00:00|13.309399999999998|\n",
      "|2017-12-05 00:00:00| 4.066799999999999|\n",
      "|2017-05-14 00:00:00|            3.9365|\n",
      "|2019-08-08 00:00:00|19.849600000000002|\n",
      "|2019-08-22 00:00:00|24.607599999999998|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_scores_time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the column names back to the original ones\n",
    "NFLX_scores_time_df = NFLX_scores_time_df.withColumnRenamed(\"0\", \"Date\")\n",
    "NFLX_scores_time_df = NFLX_scores_time_df.withColumnRenamed(\"1\", \"Sentiment_Score\")\n",
    "\n",
    "AMZN_scores_time_df = AMZN_scores_time_df.withColumnRenamed(\"0\", \"Date\")\n",
    "AMZN_scores_time_df = AMZN_scores_time_df.withColumnRenamed(\"1\", \"Sentiment_Score\")\n",
    "\n",
    "AAPL_scores_time_df = AAPL_scores_time_df.withColumnRenamed(\"0\", \"Date\")\n",
    "AAPL_scores_time_df = AAPL_scores_time_df.withColumnRenamed(\"1\", \"Sentiment_Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|   Sentiment_Score|\n",
      "+-------------------+------------------+\n",
      "|2016-08-17 00:00:00|13.309399999999998|\n",
      "|2017-12-05 00:00:00| 4.066799999999999|\n",
      "|2017-05-14 00:00:00|            3.9365|\n",
      "|2019-08-08 00:00:00|19.849600000000002|\n",
      "|2019-08-22 00:00:00|24.607599999999998|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_scores_time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframes to a timestamp format\n",
    "NFLX_scores_time_df = NFLX_scores_time_df.withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "AMZN_scores_time_df = AMZN_scores_time_df.withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "AAPL_scores_time_df = AAPL_scores_time_df.withColumn(\"Date\", to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Sentiment_Score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_scores_time_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|   Sentiment_Score|\n",
      "+-------------------+------------------+\n",
      "|2016-08-17 00:00:00|13.309399999999998|\n",
      "|2017-12-05 00:00:00| 4.066799999999999|\n",
      "|2017-05-14 00:00:00|            3.9365|\n",
      "|2019-08-08 00:00:00|19.849600000000002|\n",
      "|2019-08-22 00:00:00|24.607599999999998|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NFLX_scores_time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|   Sentiment_Score|\n",
      "+-------------------+------------------+\n",
      "|2017-05-14 00:00:00|10.664800000000001|\n",
      "|2016-08-17 00:00:00|           19.5519|\n",
      "|2017-12-05 00:00:00|38.957800000000006|\n",
      "|2019-08-08 00:00:00|48.419000000000004|\n",
      "|2019-08-22 00:00:00|60.159099999999995|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AMZN_scores_time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|   Sentiment_Score|\n",
      "+-------------------+------------------+\n",
      "|2016-08-17 00:00:00|31.851999999999997|\n",
      "|2017-05-14 00:00:00|           12.6721|\n",
      "|2017-12-05 00:00:00|           48.6196|\n",
      "|               null|            0.5859|\n",
      "|2019-08-08 00:00:00|40.425200000000004|\n",
      "+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AAPL_scores_time_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
